{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_vs_ConvNet_MnistData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maxJoRGjCeSb",
        "colab_type": "text"
      },
      "source": [
        "### ***DESCRIPTION***\n",
        "\n",
        "---\n",
        "\n",
        "MNIST DATASET: INCLUDE HANDWRITTEN IMAGES OF NUMBERS 0 TO 9.\n",
        "\n",
        "TOTAL SAMPLES: 70000\n",
        "\n",
        "TRAIN SAMPLES: 60000\n",
        "\n",
        "TEST SAMPLES:  10000\n",
        "\n",
        "**[OBJECTIVE](https://)**\n",
        "\n",
        "CLASSIFY IMAGES INTO SPECIFIC CLASS USING SINGLE LAYER PERCEPTRON\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25QlCMCFDr2U",
        "colab_type": "text"
      },
      "source": [
        "### ***QUESTION 01***\n",
        "\n",
        "---\n",
        "A. LOAD MNIST DATA FROM KERAS LIBRARY. SPLIT THE SAME AS TRAIN - TEST\n",
        "\n",
        "B. SCALE THE VALUE 0 TO 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpe6-ifOEfUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrSPsBHCPWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CumvH0Q9Exem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train= x_train/255.0\n",
        "x_test= x_test/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euwq2ixAFepo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-qzBMr3Fuwv",
        "colab_type": "text"
      },
      "source": [
        "### ***QUESTION 02***\n",
        "\n",
        "---\n",
        "A. DISPLAY FIRST 25 LABELS FROM TRAINING DATASET AND ALSO DISPLAY LABELS ALONG WITH THEM.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3aubBFaGCpy",
        "colab_type": "code",
        "outputId": "e43e8daf-a1f0-487c-ef92-404e390e555f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjXkUvKKHiUH",
        "colab_type": "code",
        "outputId": "7cd4d420-0ccb-4a7d-9c76-f336d9bb1438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "fig, axs = plt.subplots(5,5)\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        axs[i,j].imshow(x_train[i+j], label = str(y_train[i+j]))\n",
        "        axs[i,j].axis(False)\n",
        "        axs[i,j].set_title(str(y_train[i+j]), color = 'red')       "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD3CAYAAABo3V3uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOzdeXwV1d348c+ZmXvn3uwJ2UMgISEJ\nRHZBUPZFK1XRutTi0lqX5xFttbXWR0vVtv66W2ut1qWoVFREBUTc6lMXkEXZwyIBhJCELGTfbu42\nc35/XMpDFRWSe3OT3Hm/Xnm94F4458sw850z55w5R0gpsVgslkimhDsAi8ViCTcrEVoslohnJUKL\nxRLxrERosVginpUILRZLxLMSocViiXhWIrRYLBEvNIlQiA8Qwo0Q7cd+SkNST18iRBJCrECIDoQ4\njBDzwx1SryHE0GPny5JwhxJ2QtyKEJsRwoMQz4Y7nF5DiGEI8R5CtCDEAYS4JJjFh7JFeCtSxhz7\nKQxhPX3Fo4AXSAOuAv6GEMXhDanXeBTYFO4geokq4AHg6XAH0msIoQGvAauBJOAmYAlCFASrCuvR\nuCcIEQ1cCvwcKduR8iNgFXBNeAPrBYS4EmgG/hXuUHoFKZcj5UqgIdyh9CJFQCbwEFIaSPkesI4g\nXj+hTIS/QYh6hFiHENNDWE9fUAD4kXLfCZ/tACK7RShEHPBL4MfhDsXS5wjgjGAVFqpEeBcwBMgC\nngReR4i8ENXVF8QArZ/7rAWIDUMsvcmvgEVIWRnuQCy9WilwFLgTIWwIcS4wDYgKVgWhSYRSfoyU\nbUjpQcrFBJqxc0NSV9/QDsR97rM4oC0MsfQOQowGZgMPhTsUSy8npQ+4GPgmUAPcASwDgnYD1YJV\n0NeQBJqykWofoCHEUKTcf+yzUcDuMMYUbtOBHKAcISDQalYRYjhSjg1jXJbeSMoSAq3AACHWA4uD\nVXzwW4RCJCDEeQjhQAgNIa4CpgJvB72uvkLKDmA58EuEiEaIc4B5wHPhDSysngTygNHHfh4H3gDO\nC2dQYRe4ZhyASuDG4Dg2ahrZhBh57FhEIcRPgAzg2WAVH4pHYxuB4f86oB74AXDx5wYKItECwEmg\nr+NF4GakjNwWoZQupKw5/hPoPnAjZV24QwuzhUAn8D/A1cd+vTCsEfUO1wDVBK6fWcAcpPQEq3Bh\nLcxqsVginTWP0GKxRDwrEVoslohnJUKLxRLxrERosVgi3lcOy89RLg/rSMq75su9bu6hdUxOzjou\nX2Qdky/qrcfEahFaLJaIZyVCi8US8awZ62GgZWXiOiOTliE2AOIP+ogqqcRfXRPmyCyWyBSyRChs\n9uO/VrPSkU4daVMRhkSWVSKLcnCnRSFMcFS1IcqrMZpbQhVOr6HGxVF94WC857bywIilGFLh3p0X\n4nw7lwFPWYnwC4RATU7GM3IQ+taDGM3NEKEvAagJ8chBGXRmBRYtitp0EKOhMWKPRzB1PxEqKkIR\nCE1D2O1gtyF0HSM98fgfOXBJHN4MH1EJnXR26Ax5phD/3Y38q/g5qg0XUz/4IXmLhqB8uK3b4fRm\nQtdpn1nE/XcsZqazEZtQMaRk5oSneK04h6XP5iB93nCHGRyKirBpCCEw3e6uF6PrNM3J475fPsPv\nF1yDvu5TzI6OIAbaNwhdp2NyIY3Xt7N1wuMAzLx1AdFvbkd6gvamWcTqdiJUh+ZiRuk0D4ulYaRA\ny28jK7GFN4v+8YU/W+7v5PX2M3gjawTvFC2nzO/hoaOziP/Egb2iCn93g+nFhK7jnjmSu/74D2Y4\nWrEJG7VGJ/WGDVAZrVfw7Jx5OD/YjelyhTvcbtNysmkZm4ZfF8Q/v7HL5Qi7nfqRgj+Xz0bxmUGM\nsG9xzxpJ7bWdrDnzKUzsGFIiIq0lKASIE4Y15OfOh24cj24lQnPaGK57ciXnRR0hSgn0dylfMv5i\nYnJ3xTz2Ly1EdUvO4YfEVPnR692kf7oTf1v/XJpP2Oyo6ak0TB3Infe9wGxnGwoqAOvdWSzcOg9Z\nHs2uq/7C6icfYczzPyL/gV2Yffx4uHMGUDUDiPUS/3w3CtJ1GNTJrNS9vKWmByu8PqdzgEb2gGZi\nhC3cofQ4NTGR9qlDqbzUzw2j13Fz4jZ0oTFl29V41yaDhPZ8H7mvSmz/3NylOrqVCO2fHeVTdyYz\noypPmgDvOzqGMtcA7sh8h0zVy+YDORQ+vR3MQCaXhgnSxPT337Zg5zdG47m5kUXDH2KwJuBYEgSY\nG1XLsqwqSg4O5Z6as/ht+iaiCpsR9r5/sjcV2Ll00kaW7x7d5TKEruMfmsmayX/lgh3fJ21nBUYE\nPhYb08fi+lYLT+W9TL3p5+G6qZT8eBQxJfsxvP2kK+VLqMWFHLpsAAvnv8Q052ESFI2FtVPJ0pt4\ncPgrZI4MNBjcUuWKxBsZ/K7oUsuwW4nQqKll+fPTWHH2SNyddkYOPMKLQ97BI30sbh3KxjvHY2/x\nsmDYbRw9x0/0QVu/eOw7VVruYGrHqzxauIKCY4NHv6gbzYs7z2TR2YvJ1joprU8lZZvJavMsfn/d\nZnRb/7gpSFWQbGvvVhmiMJeKH5kkq06ayhNJ64y8wSQ5aRS1t7n5w4hXGaLB6o5s3lo5kcEfb8Xw\nePrvQImioubncOiyAdz1nVeY5jzMYw1ns/KVyWSud1OiCh779kwemLqcK2KOUunvxF8R3eXqupUI\npd9P9mtHaS9NQus0+XRcAb/7TiXXJmzmwffnUrA20JE74GAS8fuzUTs7iJReHm1wNgevyeLcuZsY\na2/DJQWPNI7h7b9OJk4X3GS7Gv9RJxlrIWHjERwNaXAdfDd3I68VzULd4urWIEM4qWmpdKbCQHtj\nt8oxonXm5O4BIHGHguznrZ+TqZwVze1FK5nkaOagX/BY2XQGv9bYZ8+NU2WePYIDlzm4esaHnO08\nxEN1U/nwb2eR80E1ZlkFxqQROBM9FNurcEmTj93ZDPyX0eUbQ7cHS4zSA0QfdiD9fpLtY3lr2nCu\nTdiM8InjJ67R0IjY0BgxSVBNHkDd9IEUz9nH3anv02zCHl8yizZMoejFEsziIbiqo4g52Ab7yvC7\nXDgUgYnk27F7eXTmheSWJ2NW9M09jXxDM/HmuMnSmrpchtA0vHE2zog+golJ0p5OZD/uQjkZJTqa\nQTMPMz1qP6DwfNNE6tdk4CxZH+7QQsqYPpbPrtS4e9oqxjnKWFhxEbvfKGTQizuOd41UznTyvcKP\nGGZX2ObRWLhlHgUbDmB0sc6gzCP8991Jb/JSfngAjYU2vj1tPTsyB+Kvqu6/zfeTEYK2Kfl0XNTK\ntenr2elN5NnayWw9MpCM99TA1I9PdhL1CSe9McQqdrKnl2Osjoc+mghb8pwUZJejCBOzs2unmJqV\nQeNwOxOdB6k1/NiOtmGYkXMeKVFRuGYW89iQhxms2Xm/M4Zl28dR+E4r/fkoqAV5HLhesuScx8lU\nXdxZfjGfvVTAoMU7A9eOEKipKQycWsFlcduo9MOfquaRvVjDaOr6jTeoE6qVT/aQrxVzb9E8luat\nYso3zyH9DZBt7ZguV0Tc0dX4OPw31bNs2BJ8UmH+lutJf9JB3s5KcLad0hShMxKq2KUnhTzWUPEk\nCQrijlLnjyPq0GkM/AiBousIp5O6GQMZcfkehmjw67qzoaXti9Ml+imhacjiPM77fx8yWBO0mV5+\nvncemW/ZkJu3hDu80FFU9t6WzJppDwJwX9U32LeygKy/b8V0uxGahpqdRfU3Mrku/U1iFcHPqs5l\n30uFpL3TvVZyUBOh9HnRNpXS8cthVD7t45c/fYafXzCPtj2DyV3lQmwo6fetQ9fZBTwx7BHybRpF\nr99C4RMdyG2bTykBKgj60+vfFb4kknd9xb9cnLAQiFBQByTRNDuPmll+rp/wPrcnbafKMHjrqcmk\n1vbvx8ETKVFRlJ0byw+TdqALG/fUTIHlA4h7o6T/di8JgZaWwrPnP0GSYueWylns+1MxGcvWYwqB\n0HUYWUDR47tZkvoiUYqNe4+ew/qVoxj09I5uH5egv2JnulzYN3zK/Ad+wnM/e5AN45bAOJg+6kr8\nyyaS+s/D+I9UBbva3kFRKbx/F4M1ya/qxpK6TkVuO/X9mVShgDRRRP+4WUQpXtyJKo4TPlNGD8cf\nq9Oa46CxGPwpPhTdYPXkR7ELk2bTztKms/BJlWbTT7Opk7aprV8/Dp5Iy0jnyOVDePXGP6KgcUf1\nRHb8YgzJH+zq/2/UOHTG2t2oQmX342eQsrma9gsnUD4X5k3YyoLkx8nVHICOicnLJWPJW+sKynEJ\nybvGpstFyvM7+O/q24m+o5I/D3mZ18/4B79Lm8zruRPJW6TiP1wRiqrDRnE4aL5kND9N+yMOofPC\njvHklZ3eyJ4hTUwkbx4azqB2b5+9+ys+6PDrfCtmPwkLX2D1zaOOfzc/5VUytRZsmJgIaowYltZP\n5MJ1t2Df4yR1q4+og02UXZbKFTdups10wLZPw/iv6TlqYiL1c3J59Y7fM1DTUVDYWJvDgIr2Pj/B\n/pS4PWz1OjjT7mXx/Q/SJm04hEGC4schBNs9CVT4TSbpnRzwm8Rud6Bu/CQoN8mQLbpgulxEffgp\n3vahzJt2J5dc/BG3Ja9l0KWN/CntXAr+q38lQhQFb6wgTdX41GuSuE7Hvv/g1z8SH5svtf+6VEwk\n99SeSdoTTjhc2hNRh0TmijJKOkYwZW4u47IqaPfpx797qX4C6w/nIj+LJqYCEku9qJ1+Clo6EK1H\nMVta8YwvwDOskyTFYEXr0IjoWwbwjBlC0/kuBmqB42ViIl9JRlRGwE64UmLUN/LDhxbw29sWMdnR\nwgGfyeKGs1mxcwyJG+w4miTj79rM2NQ13FN2KXFlRtDOjZAuw2W2taFt2E1u7SDeHjeM+1K38J24\nPbRNcrBm3JnILf1rW1+pBF4xPGpEE1vhx6hv+Oq/oKgoIws5dFEC11/wLiVeg9fenkj+llKM9u5N\nRg4n/5EqUv8JceUZHEwu+o/vjgID63zotY2Ihmb8NbUA/zHtoXmonfG5pVQZdv6xZwK5lPRc8GGi\npadRM0rn2uJ1GMf60X9dP46UjfWYEbAqEwTGGLJWlnOP7/v4YgSqB5wNJnnlbuyHDuMZms6FCYHX\n6/YeSSe3MXjzSkOaCLWMdDxFmbhS7aTHlgNgEwpJWt+9yE/FyqZx6A3ur7xbqSkpeEYMonKmnfEz\n96ArPi5f/QOGrurAbGnt84NK/iNVaEeqiP2S779qvpcnQZAfXcdebwbOjTGhCK/XaZidiz67jkvi\ntmEiWO+OZfkrUxhcviNiWsQA/opKUh7/4rQxQ9dxT8pmqsMLqNj2O7HVHO3yvMHPC34iVFTUuBjk\noAyqz06kdWonE3P2cnP6eygo1BsGb9edgdy6J+hV9xbb6rNI8Pi/tI9PyxlE/dQs6mZ7mD/yI+I1\nF4+uPp+hd20AiJiBga9T5UsgeWdkLDFVO9PP2hHPkKTYaTS93LtvHjl/2RWR71aflCnROkxMTBQU\nEveayMrqoBUfvEQoBMJuR01JpnX8QOrnu3hi3N84S/ehoOCTBtWGl43uwew6kkGerA1a1b2GAFUI\nflOwgvsHXo9zf9Txt2uEriOiohB2G/tvyuIH897kgpjdvNgyjqdWncuQezaEOfjeydR63f5DQafG\nxeGI9ZCmOjExaTQ1/MtTMFoPhDu0XkP6vDjX7MGQEiUEp0TQEqGamkL7xBwqvil5eMZznB/171Eu\nBROTFR0Z3PvJRWQv1ch7Y1Owqu1dJBhScqbuwrWgmdZBo0jZ1oEUgtqzosm46DALst9ntrOZDR4n\nd1dexNa1heSusu76J5OmtdCaYyM53IGE2L57h7NwxHJsQsUnwWUqpK6rD9pjX3+gOBw0zxuBKj4I\nSfndToRa9kBcxRkc+a6Xv45/hhnO/5wyct/RMax8dTKDX28if2cJmP3/v9cmVNaOfoHSYoO93jQA\nLoiqQz02gXhh7Vm8smMsQ5ZA7nsb+3x/YKiYKMh+3iCUk0YxYVIpM6MO4jLtvOVK5qdvzqegfGe4\nQ+tdbDZackP3skHXEqEQaOlpHPhzKhfk72Je4mqKbB3EK3ZAwSW93Fc7hfefm0DWO3XkHNmF0d7R\nr5Og6faQtr6JX9RN4J6UDTiERqFNJV+rw8QEFHZ44eqNN5B/bxvD6g5gdnQirST4pXJs9TSNNEgJ\ndyAh5E2wMyqukjRVp9Lv4eFDsyi8d4/VN/g50u0hY6MHbYH69X+4C04rEWrpabROyqHyGybFhZW8\nMPjvZKpeklQdBQc+afC+O4pbNt5A7iLBwE8/w6hviIxRL9OAfWVsuH8CYy4dy7OTn2aSbqAI2OJR\nuW7L94h9I4b8TY0Ynx3u1zeFYLEJP2j9/EahgE0YKCgYCLyGitHaGu6oeh3p86JvPcgbrhhmO5tx\npSokJsQH7W2b00qE/pw0jsyAP854ifGOKjJUJxs8MTxTNYXazlhKj6QRs8nJkO2dqOt34e8vGxGd\nItPtJvq9T8lrzOe2jQvwH1snUnNB5l4P+o59Xz+30ELMEZOPG3I4P25HuEMJOeeRDt6qLeb8mF2o\n/bwboLuM5mZuWzOftXP+jDGjmeaaQcS/78Ooq+t22aeVCNU2D7GHYvjpJ5f+34dHdeI+U7B1SHIP\ne7B9vD2w0ky3Q+ubzLY2lLXbSF37xe+sNuCpSSxppur1bK4u+m8GbArNo1BvISpqqFtZxNzht4PN\nJKbUTjzWaPFJSUn2KoVlZ43k9qL3eODcC7C15eB4t6nbT52nlQiN3aWkf83LIH31/VhL72GW7CW9\nBCJhqyajoZG0R9aTFu5A+oiY9/fy1w/ncPO0/+Xa8Rt4sW4q+Z/EBfZ37ob+s+aTxWLp94zWVop+\ntpdnlp5HeWcS5uBOSO7+2p0hfcXOYrFYgs1obiH7gfVUPQB5bA9Kl5Owpm9YLJZIZz0aWyyWiBea\nRChEEkKsQIgOhDiMEPNDUk9fJMRQhHAjxJJwhxJ2QtyKEJsRwoMQz4Y7nF5BiGEI8R5CtCDEAYS4\nJNwhhZ0QOkIsOpZL2hBiO0KcH8wqQtUifBTwAmnAVcDfEKI4RHX1NY8C/fRl69NWBTwAPB3uQHoF\nITTgNWA1kATcBCxBiIKwxhV+GlABTAPigYXAMoTICVYFwU+EQkQDlwI/R8p2pPwIWAVcE/S6+hoh\nrgSagX+FO5ReQcrlSLkSsGaZBxQBmcBDSGkg5XvAOiL92pGyAynvR8oypDSRcjVwCBgXrCpC0SIs\nAPxIeeL64juAyG4RChEH/BL4cbhDsfQpAjgj3EH0KkKkEcgzQVviPhSJMAb4/MuSLfClixVHil8B\ni5Cyb+7abukJpQR2M7gTIWwIcS6Bx8Go8IbViwhhA54HFiPl3mAVG4p5hO1A3Oc+iwMiYBuuLyHE\naGA2MCbcoVh6MSl9CHEx8AhwF7AZWAZExjLdX0cIBXiOwPjDrcEsOhSJcB+gIcRQpNx/7LNRBLEZ\n2wdNB3KA8mObmscAKkIMR8qxYYzL0ttIWUKgFRggxHpgcdji6S2EEMAiAgOwc5HSF8zig58IpexA\niOXALxHiBmA0MA84O+h19R1PAktP+P1PCCTGm8MSTW8RGCXVAJXAjcFBoH85AtZt+xJCjCTQmFCA\nBUAG8Gw4Q+ol/gYMA2YjZWewCw/V9JkFgJNAf8eLwM1IGbktQildSFlz/CfQfeBGyu6vH9S3LQQ6\ngf8Brj7264VhjSj8rgGqCVw7s4A5SBnZj8ZCDAb+i0CjqgYh2o/9XBW0KqxX7CwWS6SzXrGzWCwR\nz0qEFosl4lmJ0GKxRDwrEVosloj3ldNn5iiXh3Uk5V3z5V63nY11TE7OOi5fZB2TL+qtx8RqEVos\nlohnJUKLxRLxQr5niZaVieuMTFqG2ACIP+gjqqQSf3VNqKu2WCyWUxLSRKjGxVF94WC857bywIil\nGFLh3p0X4nw7lwFPWYnwPwiBmpyMZ+Qg9K0HMZqbIYInu6sJ8chBGXRmBRYtitp0MLBlYwQfE8sx\nioqwaSgJ8YgoJ/gN/JVHunVuhCwRCl2nfWYR99+xmJnORmxCxZCSmROe4rXiHJY+m4P0eUNVfc85\n9p8ihMB0u7tejK7TNCeP+375DL9fcA36uk8xOzqCGGjfIXSdjsmFNF7fztYJjwMw89YFRL+5HemJ\n7LfNIpnQNJT4OBiQiJEQRe2YGFqHgK1VkLvIh7+mtstlhyQRCl3HPXMkd/3xH8xwtGITNmqNTuoN\nG6AyWq/g2TnzcH6wG9PlCkUIPUbLyaZlbBp+XRD//MYulyPsdupHCv5cPhvFZwYxwr7HPWsktdd2\nsubMpzCxY0iJiLSWoBAgTujCl587JyLseAibHVGcz747da4s3sxNSRtIU3UAPnI7uDHzBobe0ksS\nobDZUdNTaZg6kDvve4HZzjYUVADWu7NYuHUesjyaXVf9hdVPPsKY539E/gO7MNv67lKF7pwBVM0A\nYr3EP9+NgnQdBnUyK3Uvb6npwQqvT+ocoJE9oJkYYQt3KD1OTUykfepQKi/1c8PoddycuA1daEzZ\ndjXetckgoT3fR+6rEts/N4c73JBT01JpOyeXhvkdvD3hcdJUHQWFDZ4Y3u1IJ9vWwHi9hZ/OWM3K\nM2cgN+/qUj1BTYSd3xiN5+ZGFg1/iMGagGNJEGBuVC3LsqooOTiUe2rO4rfpm4gqbEbY+/bJ3lRg\n59JJG1m+e3SXyxC6jn9oJmsm/5ULdnyftJ0VGBH6WGxMH4vrWy08lfcy9aafh+umUvLjUcSU7Mfw\n9oOulK+gFhdy6LIBLJz/EtOch0lQNBbWTiVLb+LB4a+QOTLQYHBLlSsSb2Twu6LftgzVuDiqrzmD\n2Iuq+Xne04ywN5F0rAW4oiOJ3/9hPgn7PRy8zManl/yVsc4ylqY40btYX9Cmz2i5g6kdr/L/CldQ\nYLOjCxu/qhtL0Xs3sM5to9bwU1qfSso2k9VvnYWCQLf1/WXnpCpItrV3qwxRmEvFj0ySVSdN5YnQ\n2fW+xr5MThpF7W1u/jDiVYZosL4zm7dWTkT7+NP+PXikqKgFeRy6bAB3fecVpjkP81jD2Uz424/Z\n9ZORvHX7dK5773o2uQeRqzmIFX78FdHhjjqkOqYVoX/zKH8rfIGzHW3Hk+A7rnh+/7v5pL1Vjl7e\niOIOzI+2Y3YrmwWlRagNzubgNVmcO3cTY+1tuKTgkcYxvP3XycTpgptsV+M/6iRjLSRsPIKjIQ2u\ng+/mbuS1olmoW1zdGmgIFzUtlc5UGGhv7FY5RrTOnNw9ACTuUJD9vOXzZSpnRXN70UomOZo56Bc8\nVjadwa819slz43SYZ4/gwGUOrp7xIWc7D/FQ3VQ+/NtZ5HxQjVlWgTFpBM5ED8X2KlzS5GN3NgP/\nZfTfGwNQM0HlvwZtJd+m4TJ9bPZE8XrzaN5+ZSI5b5fhr65FjCrCiDWCUl+3E6GaPIC66QMpnrOP\nu1Pfp9mEPb5kFm2YQtGLJZjFQ3BVRxFzsA32leF3uXAoAhPJt2P38ujMC8ktT8as6Ht7GvmGZuLN\ncZOlNXW5DKFpeONsnBF9BBOTpD2dSH/fbymfLiU6mkEzDzM9aj+g8HzTROrXZOAsWR/u0ELKmD6W\nz67UuHvaKsY5ylhYcRG73yhk0Is7jnePVM508r3CjxhmV9jm0Vi4ZR4FGw4QnBTQOzmPCl4sO5Ot\nrYPYXZdOc1UcsQc0cl44GBgdlhJfkgNbXHAaDd1LhELQNiWfjotauTZ9PTu9iTxbO5mtRwaS8Z4a\nmP7xyU6iPoGTjYPGKnayp5djrI6HPpgIW/KcFGSXowgTs7Nrh1LNyqBxuJ2JzoPUGn5sR9swzP57\npz8ZJSoK18xiHhvyMIM1O+93xrBs+zgK32mlPx8JtSCPA9dLlpzzOJmqizvLL+azlwoYtHhn4NoR\nAjU1hYFTK7gsbhuVfvhT1TyyF2sYTV2/+fYFmUv301o9hP1RA0jb3UpG+SGMujpObCK4kzSS4luC\nUl+3EqEaH4f/pnqWDVuCTyrM33I96U86yNtZCc42TqVdc0ZCFbv0pO6EETaeJEFB3FHq/HFEHTqN\nQR8hUHQd4XRSN2MgIy7fwxANfl13NrS0fXGqRD8mNA1ZnMd5/+9DBmuCNtPLz/fOI/MtG3LzlnCH\nFzqKyt7bklkz7UEA7qv6BvtWFpD1962YbjdC01Czs6j+RibXpb9JrCL4WdW57HupkLR3+ncrGcCo\nqyP6lcBOFhIwACU29tgeTgGtuSozUg9jSEmz6UTxhWlCtevsAp4Y9gj5No2i12+h8IkO5LbNp5QA\nFQT95VXnCl8Sybu+4l99wn8eQkEdkETT7DxqZvm5fsL73J60nSrD4K2nJpNa2/9P8hMpUVGUnRvL\nD5N2oAsb99RMgeUDiHuj5KRPEf2CEGhpKTx7/hMkKXZuqZzFvj8Vk7FsPaYQCF2HkQUUPb6bJakv\nEqXYuPfoOaxfOYpBT+/ov8flK6gJ8Ry6vRi/Uwa2vAeSR9QyLa6Uw34/vzh4Ifb/3dbl8rueCBWV\nwvt3MViT/KpuLKnrVOS2U9+fSRUKSBNF9P2HnyjFiztRxXHCZ8ro4fhjdVpzHDQWgz/Fh6IbrJ78\nKHZh0mzaWdp0Fj6p0mz6aTZ10ja19etHwc/TMtI5cvkQXr3xjyho3FE9kR2/GEPyB7v6/1s1Dp2x\ndjeqUNn9+BmkbK6m/cIJlM+FeRO2siD5cXI1B6BjYvJyyVjy1rr6/3E5geJwILIzqZuSRtZ3D7I9\n/2EANFTMY1dKtdHJ7WWX4PiRE9Pseq9plxKh4nDQfMlofpr2RxxC54Ud48krO72RPUOamEjePDSc\nQe3ePnmXU3zQ4df5Vsx+Eha+wOqbRx3/bn7Kq2RqLdgwMRHUGDEsrZ/Ihetuwb7HSepWH1EHmyi7\nLJUrbtxMm+mAbZ+G8V/Ts9TEROrn5PLqHb9noBaYJLuxNocBFe19eoL9KXN72Op1cKbdy+L7H6RN\n2nAIgwTFj0MItnsSqPCbTNI7OeA3id3uQN34SUTcKIXNDqMLKb3Fzq8nLWe4Xo2KZIM7ml8duoC/\n5L9ErqZiEyoKMDyuhtfmDiZkLAYAACAASURBVCVrv97lVzC71iJUFLyxgjRV41OvSeI6Hfv+g1//\nSKyoqPk57L8uFRPJPbVnkvaEEw6XdimMcMtcUUZJxwimzM1lXFYF7b7/m875Uv0E1h/ORX4WTUwF\nJJZ6UTv9FLR0IFqPYra04hlfgGdYJ0mKwYrWoRE1WuwZM4Sm810M1ALHzMREvpKMqNwX5sh6gJQY\n9Y388KEF/Pa2RUx2tHDAZ7K44WxW7BxD4gY7jibJ+Ls2MzZ1DfeUXUpcmRER54ew2XHPHoV6Ry3P\nDVnJh+3DuPvDy4jbbSNlhxsUwT8enMTtAz4iSdVJUuxcn7iB+GtdrDw0m7jXd3RpulWXH42lAgoK\nR41oYiv8GPUNX/0XFBVlZCGHLkrg+gvepcRr8NrbE8nfUorR3r0JyeHiP1JF6j8hrjyDg8lF//Hd\nUWBgnQ+9thHR0Hz8hfATG+/NQ+2Mzy2lyrDzjz0TyKWk54IPIy09jZpROtcWr8M4Nhfu1/XjSNlY\nj9kcnFHA3k76vGStLOce3/fxxQhUDzgbTPLK3dgPHcYzNJ0LEwKv1+09kk5uY/+fWyo0jfrvjiPm\nimoeyl9GhT+Bv28/hyFLTRyfVSDbOyj9eQF3xn6KKgS/rR9FaXsa05NKuS6+hI9/mMuuEWPI/mcn\ntkYXGCaipe2UFmPo9jzClU3j0BvcX3m3UlNS8IwYROVMO+Nn7kFXfFy++gcMXdWB2dLapyeG+o9U\noR2pIvZLvv+qXgtPgiA/uo693gycG2NCEV6v1DA7F312HZfEbcNEsN4dy/JXpjC4fEdEtHr+zV9R\nScrjX5w2Zug67knZTHV4ARXbfie2mqP9et6g4nDgmj2S9KvLeGzIyyxqOot/bJ1Ixjs29G2lyJQk\njlySzW/mPk+m1sKthy+i5L0CnDWCT3KLWDW+jIvSdjD24gr+PvgcaE9E6RSkbk4mdmkPJMJt9Vkk\nePxf2sen5QyifmoWdbM9zB/5EfGai0dXn8/QuzYARESfx9ep8iWQvDNylpeqneln7YhnSFLsNJpe\n7t03j5y/7IrY96u/wJRoHSYmJgoKiXtNZGV1uKMKGaHryOI8mm5oY1X+cp5sHsWyFdMYstaDvbYZ\nz5gh1EzUufGqN7ELgzs+u5zWxQPJ+9dhjKP1ZKan0jo2m99NG8y1s9awdOoT2DFZ1jyelU2Tv7SR\ncqKuJ0IBqhD8pmAF9w+8Huf+qOOvhgldR0RFIew29t+UxQ/mvckFMbt5sWUcT606lyH3bOhytf2V\nqfW6fXZCQo2LwxHrIU11YmLSaGr4l6dgtB4Id2i9hvR5ca7ZgyElSgScFkp2JqVXx7Jn/CPUGgZP\nLzuPrDVu3Mk2Dp+fwLTJu1iS+TZtUnDhcz9hyNImEvdvxX9sYMRfUYmzopL812D1ddNYOnccdpsf\n7c0Ect88fErT+bqeCCUYUnKm7sK1oJnWQaNI2daBFILas6LJuOgwC7LfZ7azmQ0eJ3dXXsTWtYXk\nrrLu+p+XprXQmmMjOdyB9IB99w5n4Yjl2ISKT4LLVEhdV9+vH/tOl+Jw0DxvBKr4INyh9AjP4CSe\nn/coAA4h+N7l7xJzpZvpUfvI1QIrWB32K1y09A7y7t+E+RXdJ0nPbCDpmf/7/al2tHT70dgmVNaO\nfoHSYoO93jQALoiqQz02iXhh7Vm8smMsQ5ZA7nsb+3R/YKiYKMgIuPPLSaOYMKmUmVEHcZl23nIl\n89M351NQvjPcofUuNhstuf3jZYNToXhN1ruGMkYvJV6xc3tSYAGSBxtG8+zuiehbohn4bhN5e7aE\nrA+5S4nQdHtIW9/EL+omcE/KBhxCo9Cmkq/VYWICCju8cPXGG8i/t41hdQcwOzqRVhI8qRxbPU0j\nDVLCHUiIeRPsjIqrJE3VqfR7ePjQLArv3WP1DX6OdHvI2OhBW6B+/R/uB7RNn/LuVWex6JvfwHOG\nC7NRJ/sdSXRpA/lNVciODky3B7oxYfprY+jS3zIN2FfGhvsnMObSsTw7+Wkm6QaKgC0eleu2fI/Y\nN2LI39SI8dnhkP4D+gOb8IMWATcJBWzCQEHBQOA1VIzW1nBH1etInxd960HecMUw29mMK1UhMSG+\n375VYrrdiD2fkdOQgoyLBm8LsrYeo8PVY7mjy4/GpttN9HufkteYz20bF+A/tk6k5oLMvR70Hfu+\nfm5hhIs5YvJxQw7nx+0Idyg9wnmkg7dqizk/ZhdqBHQFdIfR3Mxta+azds6fMWY001wziPj3fRh1\ndeEOLSSkzxvYiS5MutVHaLa1oazdRuraL35ntQG/XmJJM1WvZ3N10X8zYFP/fwwSFTXUrSxi7vDb\nwWYSU2onHmu0+KSkJHuVwrKzRnJ70Xs8cO4F2NpycLzbFFFzLXtKyDd4t3w5s2Qv6SUQKVs1GQ2N\npD2ynrRwB9JHxLy/l79+OIebp/0v147fwIt1U8n/JC6wv7MlqCJnaMpi6WOM1laKfraXZ5aeR3ln\nEubgTkjum2t39nZWi9Bi6cWM5hayH1hP1QOQx3aryylEhDWlxWKxRLrQPhoLMRQh3AixJKT19AVC\n3IoQmxHCgxDPhjucXkGIYQjxHkK0IMQBhLgk3CGFnRA6QixCiMMI0YYQ2xHi/HCHFVY9cO2Euo/w\nUWBTiOvoK6qAB4Cnwx1IryCEBrwGrAaSgJuAJQhRENa4wk8DKoBpQDywEFiGEDlhjCncQn7thC4R\nCnEl0Az8K2R19CVSLkfKlYA1uTKgCMgEHkJKAynfA9YB14Q3rDCTsgMp70fKMqQ0kXI1cAgYF+7Q\nwqYHrp3QJEIh4oBfAj8OSfmW/koAZ4Q7iF5FiDSgADj1DYEspy1ULcJfAYuQsu9tVmzpKaUEFvK+\nEyFsCHEugcfBqPCG1YsIYQOeBxYj5d5wh9OfBX/6jBCjgdnAmKCXbek/pPQhxMXAI8BdwGZgGRA5\nK9R+FSEU4DnAC9wa5mj6vVDMI5wO5ADlx/bzjQFUhBiOlGNDUJ+lr5KyhEArMECI9cDisMXTWwR2\nMV8EpAFzkdIX5oj6vVAkwieBpSf8/icEEuPNIair7wiMkmqASuDG4AD8SBm5L44KMRLYR6CLZgGQ\nATwbzpB6ib8Bw4DZSNkZ7mDCrgeuneD3EUrpQsqa4z/QDriRsn8um3HqFgKdwP8AVx/79cKwRhR+\n1wDVBPoKZwFzkDKyH42FGAz8FzAaqEGI9mM/V4U5snAK+bVjvVlisVginrXogsViiXhWIrRYLBHP\nSoQWiyXiWYnQYrFEPCsRWiyWiPeV8wjnKJeHdUj5XfPlXrfFj3VMTs46Ll9kHZMv6q3HxGoRWiyW\niGclQovFEvGsPUssFkuvpg5Ion1KPg3DNByNkvRVh/DX1gV18/fwJEIhUJOT8YwchL71IEZzM0To\nGy5qQjxyUAadWbEARG06GNiuMUKPh+VzFBVh01AS4hFRTvAbgY3QI+X8UFT8Bdlwcx27R6xgaVsi\nTx66FL21DbOjI3jVnG5QQtdRHI7uVarrNM3J484nluAen4cSFZlL0Aldp2NyIZW/UHj374/z7t8f\np21KPsJuD3doljATmoY6IAk1Pwc5qoDai/PY99+ZHLp2EFpaarjD6zFqfBwt+VH8sWAZPmlwScxR\nWnJsiCDnjNNqEWo52bSMTcOvC+Kf39jlSoXdTv1IwZ/LZ6P4zC6X09e5Z42k9tpO1pz5FCZ2DCkR\nkXKnP5EQIE64J8vPnRMRdkyEzY4ozmffnTpXFm/mpqQNpKk6AB+5HdyYeQNDb6kNc5Q9w2zvIK7c\nzS8PX8SKoatDVs9pJUJ3zgCqZgCxXuKf70atug6DOpmVupe31PRuFNS3dQ7QyB7QTIywhTuUsFAT\nE2mfOpTKS/3cMHodNyduQxcaU7ZdjXdtMkhoz/eR+6rE9s/N4Q435NS0VNrOyaVhfgdvT3icNFVH\nQWGDJ4Z3O9LJtjUwXm/hpzNWs/LMGcjNu8IdcshJnxdbvYt91akwNHT1nFYibCqwc+mkjSzfPbrL\nFQpdxz80kzWT/8oFO75P2s4KjCA+6/cVxvSxuL7VwlN5L1Nv+nm4biolPx5FTMl+DK833OGFnFpc\nyKHLBrBw/ktMcx4mQdFYWDuVLL2JB4e/QubINgDcUuWKxBsZ/K7oty1DNS6O6mvOIPaian6e9zQj\n7E0kHWsBruhI4vd/mE/Cfg8HL7Px6SV/ZayzjKUpTvQwx90TFIcDd1YsU3JDu1PBaSVCqQqSbe3d\nqlAU5lLxI5Nk1UlTeSJpnTXdKq8vkpNGUXubmz+MeJUhGqzuyOatlRMZ/PFWDI+n317wACgqan4O\nhy4bwF3feYVpzsM81nA2K1+ZTOZ6NyWq4LFvz+SBqcu5IuYolf5O/BXR4Y46pDqmFaF/8yh/K3yB\nwZrAJgIp7h1XPL//3XzS3ikHuw3FHXh6smNGzsQ3mw1vnMqUhH3HP2oZCgPyM1Hdbsy2tqBUc8qH\nU01LpTMVBtobu1WhEa0z51h2T9yhICOg9fN5lbOi+UHRB0xyNHPQD4+VTWfwa42Ybnf/ToKAefYI\nShekcMW3PuRs5yEeqpvKPx89h5yXqtHWlqB6TJyJnRTbq3BJLx+7sxn4L6NfH5eaCSpXDNpKvk3D\nJ002uHXuqTmLu5/+HmlvH8ZfVYMRH40RG7zpIn2FdHuIqvbwxKEpxz+75twPOTI9GjEweN1qp9wi\n9A3NxJvjJktr6nJlQtPwxtk4I/oIJiZJezqR/shaqV6JjmbQzMNMj9oPKDzfNJH6NRk4S9aHO7SQ\nM6aP5bMrNe6etopxjjIWVlzE7jcKGfTijuPdI5UznXyv8COG2RW2eTQWbplHwYYD9OcU4DwqeLHs\nTLa2DmJ3XTrNVXHEHtDIeeEg/ppakBJfkgNbXOQ1GqTPi3agCvfr+TAy8Nn/JO/g2ZxzMOKdQavn\nlBNhS56TguxyFGFidnZt+qGalUHjcDsTnQepNfzYjrZhmP33Tv95SlQUrpnFPDbkYQZrdt7vjGHZ\n9nEUvtNKfz8KakEeB66XLDnncTJVF3eWX8xnLxUwaPHOwHwwIVBTUxg4tYLL4rZR6Yc/Vc0je7GG\n0dT1m29fkLl0P63VQ9gfNYC03a1klB/CqKvjxCaCO0kjKb4lbDGGk1F7lMw3dJSfCULVJ3DKGc2T\nJCiIO0qdP46oQ6cxyikEiq4jnE7qZgxkxOV7GKLBr+vOhpa2L06V6KeEpiGL8zjv/33IYE3QZnr5\n+d55ZL5lQ27eEu7wQktR2XtbMmumPQjAfVXfYN/KArL+vhXT7Q7MmcvOovobmVyX/iaxiuBnVeey\n76VC0t6JgJZyXR3RrwS29JGAASixscc2swtozVWZkXoYQ0qaTSeKr7/fOr9IFQqmPPZsEOTlJE67\naVfhSyJ511c8zp7wn4dQUAck0TQ7j5pZfq6f8D63J22nyjB466nJpNb2/5P835SoKMrOjeWHSTvQ\nhY17aqbA8gHEvVFCv74VCIGWlsKz5z9BkmLnlspZ7PtTMRnL1mMKgdB1GFlA0eO7WZL6IlGKjXuP\nnsP6laMY9PSO/n1svoSaEM+h24vxO+XxCz55RC3T4ko57Pfzi4MXYv/fbeENMgx80sD89xkR5PvA\naSfCKMWLO1HlxHdLlNHD8cfqtOY4aCwGf4oPRTdYPflR7MKk2bSztOksfFKl2fTTbOqkbWrr94+D\n/6ZlpHPk8iG8euMfUdC4o3oiO34xhuQPdgX1NaFey6Ez1u5GFSq7Hz+DlM3VtF84gfK5MG/CVhYk\nP06u5gB0TExeLhlL3lpXZBybYxSHA5GdSd2UNLK+e5Dt+Q8DoKFiHrtSqo1Obi+7BMePnJhBfM/W\nchqJUPFBh1/nWzH7SVj4AqtvHnX8u/kpr5KptWDDxERQY8SwtH4iF667BfseJ6lbfUQdbKLsslSu\nuHEzbaYDtn0akn9Qb6MmJlI/J5dX7/g9A7XABNmNtTkMqGgP2tB/r+f2sNXr4Ey7l8X3P0ibtOEQ\nBgmKH4cQbPckUOE3maR3csBvErvdgbrxk4i4UQqbHUYXUnqLnV9PWs5wvRoVyQZ3NL86dAF/yX+J\nXE3FJlQUYHhcDa/NHUrWfh3pieydT4PplBNh5ooySjpGMGVuLuOyKmj3/d90zpfqJ7D+cC7ys2hi\nKiCx1Iva6aegpQPRehSzpRXP+AI8wzpJUgxWtA6NmNFiz5ghNJ3vYqAWOF4mJvKVZETlvq/5m/2E\nlBj1jfzwoQX89rZFTHa0cMBnsrjhbFbsHEPiBjuOJsn4uzYzNnUN95RdSlyZERHnh7DZcc8ehXpH\nLc8NWcmH7cO4+8PLiNttI2WHGxTBPx6cxO0DPiJJ1UlS7FyfuIH4a12sPDSbuNd3BKZcRQibUPl3\n12h+fg2daRkEa9z4lBOh/0gVqf+EuPIMDiYX/cd3R4GBdT702kZEQ3NgyB/+Y8pD81A743NLqTLs\n/GPPBHIpCUb8vZqWnkbNKJ1ri9dhHJsH9+v6caRsrMdsjpwRQOnzkrWynHt838cXI1A94GwwySt3\nYz90GM/QdC5MCLxet/dIOrmN/X+aiNA06r87jpgrqnkofxkV/gT+vv0chiw1cXxWgWzvoPTnBdwZ\n+ymqEPy2fhSl7WlMTyrluvgSPv5hLrtGjCH7n53YGl1gmIiWtuPXXn9knDCwesfgf/KTousZtD4F\no66u22WfVh+h/0gV2pEqYr/k+6/qtfAkCPKj69jrzcC5MeZ0qu2zGmbnos+u45K4bZgI1rtjWf7K\nFAaX74iIFs+J/BWVpDxe+YXPDV3HPSmbqQ4voGLb78RWc7RfzxtUHA5cs0eSfnUZjw15mUVNZ/GP\nrRPJeMeGvq0UmZLEkUuy+c3c58nUWrj18EWUvFeAs0bwSW4Rq8aXcVHaDsZeXMHfB58D7YkonYLU\nzcnELu2nidDr4+GmfG5JLAVghrOdzhGdeIuzUT/o4UQYDFW+BJJ3RkbfRu1MP2tHPEOSYqfR9HLv\nvnnk/GVXRL5b/aVMidZhYmKioJC410RWVoc7qpARuo4szqPphjZW5S/nyeZRLFsxjSFrPdhrm/GM\nGULNRJ0br3oTuzC447PLaV08kLx/HcY4Wk9meiqtY7P53bTBXDtrDUunPoEdk2XN41nZNPlLGyl9\nnXR7eGTDLG6ZW3r8s3E55XxWUEDyB90vPywLs5par9tTJujUuDgcsR7SVCcmJo2mhn95CkbrgXCH\n1qtInxfnmj0YUqL0/9MCJTuT0qtj2TP+EWoNg6eXnUfWGjfuZBuHz09g2uRdLMl8mzYpuPC5nzBk\naROJ+7fiPzYw4q+oxFlRSf5rsPq6aSydOw67zY/2ZgK5bx6mvz5nSK+XmP02XKaPKMWGgkKs5sHQ\nBShqt1er7vFEmKa10JpjI7mnK+5h++4dzsIRy4938LpMhdR19f36ka8rFIeD5nkjUMUH4Q6lR3gG\nJ/H8vEcBcAjB9y5/l5gr3UyP2keupgJw2K9w0dI7yLt/E+ZXdKEkPbOBpGf+7/f9NQkCmB0dZP7x\nY16+Lp+rY8tQBCwa9BFjZg9EfXswxv6D3Sq/xxOhiYLs53d+OWkUEyaVMjPqIC7TzluuZH765nwK\nyneGO7Tex2ajJTdSllIBxWuy3jWUMXop8Yqd25P2APBgw2ie3T0RfUs0A99tIm/PlojrR/5a0mRJ\n+VlcPHw/8ULgk2AGKZn0eCLMsdXTNNIgpacr7kHeBDuj4ipJU3Uq/R4ePjSLwnv3WH2DJyHdHjI2\netAWqOEOpUdomz7l3avOYtE3v4HnDBdmo072O5Lo0gbym6qQHR2Ybk9QNybqN6Skc2k6DfcJ4oN8\n7+zxRGgTftD6+VRZBWzCQEHBQOA1VIzW1nBH1StJnxd960HecMUw29mMK1UhMSG+375VYrrdiD2f\nkdOQgoyLBm8LsrYeo8NlJb9TMGBnG8tazuSmxE9IVsOw+kx3xRwx+bghh/PjdvRUlWHjPNLBW7XF\nnB+zC7WfdwMEg9HczG1r5rN2zp8xZjTTXDOI+Pd9QZkf1htJnzewE53ltCmfHeGNP01jedJ0TA0S\nPjOgtvtvqfVYIkwsaabq9WyuLvpvBmzq349BoqKGupVFzB1+O9hMYkrtxGONFn8pKclepbDsrJHc\nXvQeD5x7Aba2HBzvNln9ZJb/YDQ1kbh4w39+FoRyeywRmiV7SS+BSNiqyWhoJO2R9aSFO5A+JOb9\nvfz1wzncPO1/uXb8Bl6sm0r+J3GBPZ4tlhCLnOE6S69mtLZS9LO9PLP0PMo7kzAHd0JyUrjDskSI\nsEyotlhOxmhuIfuB9VQ9AHlst+ZcWnqMkP14UxyLxWI5FcF/NBbiVoTYjBAehHg26OX3VUIMQ4j3\nEKIFIQ4gxCXhDimshNARYhFCHEaINoTYjhDnhzussLOun5MTYglCVCNEK0LsQ4gbgll8KPoIq4AH\ngKdDUHbfJIQGvAasBpKAm4AlCFEQ1rjCSwMqgGlAPLAQWIYQOWGMqTewrp+T+w2Qg5RxwEXAAwgx\nLliFBz8RSrkcKVcCDUEvu+8qAjKBh5DSQMr3gHXANeENK4yk7EDK+5GyDClNpFwNHAKCdnL3Sdb1\nc3JS7kbKfy9bJY/95AWreGvUOHwEcEa4g+g1hEgDCoDd4Q7F0ksJ8RhCuIC9QDXwZrCKthJhzygl\nsJD3nQhhQ4hzCTwSRoU3rF5CCBvwPLAYKfeGOxxLLyXlAiAWmAIsB4K2sKmVCHuClD7gYuCbQA1w\nB7AM+OKSzZFGCAV4DvACt4Y5GktvF+ha+ggYCNwcrGKteYQ9RcoSAq3AACHWA4vDFk9vENjBfBGQ\nBsw9dsOwWE6FRq/uIxRCQwgHoAIqQjiOjZpGNiFGHjsWUQjxEyADeDbMUYXb34BhwIVI2RnuYHoF\n6/r5IiFSEeJKhIhBCBUhzgO+A/wrWFWE4tF4IdAJ/A9w9bFfLwxBPX3NNQQ6eI8Cs4A5J4yCRR4h\nBgP/BYwGahCi/djPVWGOLNys6+eLJIHH4EqgCfgjcDtSrgpWBdabJRaLJeJZgyUWiyXiWYnQYrFE\nPCsRWiyWiGclQovFEvG+clh+jnJ5WEdS3jVf7nU7fljH5OSs4/JF1jH5ot56TKwWocViiXhWIrRY\nLBEvsmesWyy9jDogifYp+TQM03A0StJXHcJfW2fteRxiPZoI1YR45KAMOrNiAYjadDCwS5k1qdsC\noKgIm4aSEI+IcoLfCOz/Gynnh6LiL8iGm+vYPWIFS9sSefLQpeitbf12w/uvIzQNoesIm4ZITKCj\nKPXkf06CXt+J3LK7S+dLjyVCoet0TC6k8fp2tk54HICZty4g+s3tSE/kvmlmCZzsSnwcDEjESIii\ndkwMrUPA1irIXeTDX1Mb7hB7hBofR0N+FI8U/B2fVLgk5ih/yLGRti0KIi0RKiqK04EYmEFnTgL+\nKJXaCQqfXvsoACb/mexaTDdX7fs22ncz8VfVnHYLuscSoXvWSGqv7WTNmU9hYseQEhEpd/p/EwLE\nCd2y0vzP7yPteADCZkcU57PvTp0r/397Zx4nRXXt8e+tqt6GWZvZmGH2YRhB9n2bQcSNKLigEUSN\ncXlPzVPyjFmMicmL771ogoaYZ4gRhYiKiICoKBKNIIIisgnCDAyzwuzMxvT0VnXfHz3iRoxKz/TY\nXd/Ph8+Hqe6ue7q66nfPPfece4fu5BbndlJUGwBb3XZuTruJQbdHhhAaJzuJrXLzX5WzWTvo5VCb\nE1KUswdRfZGTxHOP8585z9BP8TDV7sbontYw+OyzE6NYWTt4DeMW30DWT6wYFdVIv/8rt9drQtjV\nXyOjfyvRwtJbTfYZ1IQEThYNouYKPzeNfIdbE3ZjExrTdi/A+3YiSDiZ7yPnBYnl9Z2hNrdXUFOS\n6ZiSQ/P8Tl4bv4QU1YaCwnZPNJs6U8mwNDPO1saPz3mZdWPPQe7cH2qTexzp82JpclFamwyDQm1N\naDl0WywPzHiaOf2avvJnVCHYNXEZxRP+A2dzK3pLy1f+bK8IoT59NK7L2/hr3vM0GX4WNxax7z9H\nEL3vMLrX2xsmhAx16GDK5/bn3vnPUeyoJF7RuLe+iHRbC4uGrCZteAcAbqlyVcLNZG0SYe0ZqrGx\n1F57NjGza/lF3hMMs7bg7PYA13Y6efB384k/7OHoXAsHL/sTox0VrExyYAux3b2BYrfjTo9hWo65\nSHf8hxpbxxYwp18TLsPHZncyP9x8dWCDi+7HIyOjme9nvcO8mGNn3F6PC6GcNIL6O938btgL5Grw\ncmcGr66bSNZ7u9A9nvB96BUVNT+b8rn9+cm81RQ7Knm0eTLrVk8lbZubfarg0e/O4P6iNVwV3UCN\nvwt/db9QW93jdBYXYvtOA38e/AxZmsAiAhK30RXHgw/MJ2VjFVgtKO5UAKwYkZPkZbHgjVWZFl96\n6lDbIOifn4bqdmN0dITQuN4lbX0V+46OpChpLMKQWFySs/Z/1jvUnTHcP/dyRs/9A4MtKgA3Vp6H\nc0/L175WPS6ENef2Y2HhOibZWznqFzxaMZ2sF09guN093XRIMSYP48hcOwvO2cxkRzkPNxax+c8T\nyH6rFqOiGn3SMBwJHoZaj+OSBu+5Mxj4hh6+HUM3deNV/i1zF/kWDZfhY6cnipdaR/La6olkv1aB\nv7YeMaIQPSby0kWk20NUrYe/lE/jmuGrALj2/M2sOjGdrNZUOBg5QuivOYaj+QRR1kAoTfr86J+b\nMFKGF2JYorALHVDRpeT96kzyGmq/VnwQelgIlX79yJxRyfSow4DC0y0TadoyAMe+bT3ZbMjRp4+m\n7GqNnxWvZ4y9gnurZ3PglcFkPrv31I9ZM8PB9wZv5Syrwm6Pxr0fzKFg+xHC/fF3NAierRjLrvZM\nDjSm0no8lpgjGtnPHA3MDkuJz2nHEhveIZPTIX1etCPHcb+UD8MDx36auJdl2VPQ4xyhNa63kRLD\n5QLX6V9WC/KoLXKS6sv46AAAG99JREFUNeQYSconVXOOrdHIbzDD3mNCqERF4ZoxlEdzF5OlWflH\nVzSr9oxh8MZ2wtnnUQvyOHKjZMWUJaSpLu6uupSy5wrIXP5hIBdMCNTkJAYWVTM3djc1fnjo+Bwy\nlmtfK7j7bSVt5WHaa3M5HNWflAPtDKgqR29s5NP9t9up4YxrC5mNoUSvbyDtFRvKzwWRExP46mgD\n0/FmJ1E9PYqEqXXcmf0GUUrAa6z0+0l7tgS96+vv+tAjQig0DTk0jwv+ezNZmqDD8PKLQ3NIe9WC\n3PlBTzTZN1BUDt2ZyJbiRQDcd/xCStcVkP74Lgy3G6FpqBnp1F6Yxg2pG4hRBD8/fj6lzw0mZWN4\ne8kfozc20m91IxCIeeuAEhPTvY9TgPYclXOSK9GlpNVwoPjCues8PapQMGT3+KDPLZ3QCygqit2G\ncNhBC8iUEIKKa7OYN+9N5sXtZIBqRRUCt/RzxKfywLFLkB5v30moVqKiqDg/hjuce7EJC/fUTYM1\n/Yl9Zd/nsn/CCCHQUpJYdtFfcCpWbq85l9KHhjJg1TYMIRA2GwwvoHDJAVYkP0uUYuGXDVPYtm4E\nmU/sDd/r8i9Q4+MoXzgUv0OeeuATh9VTHFtCpd/Pr49egvXvu0NrZAjwSf2TXLnI6wdQczOpvWAA\nnVM7Kc49AoAiDNalPwKA0Z1H0GZ4eaxlDM8vnUH6hjqMjqPfqL2gC6E2IJVjV+byws2/R0HjrtqJ\n7P31KBLf2h/+ZUJ2G6OtblShcmDJ2STtrOXkJeOpmgVzxu/itsQl5Gh2wIaBwfP7RpP3tiv8r8vn\nUOx2REYajdNSSL/+KHvyFwOgoZ6qGKjVu1hYcRn2HzowzDrbyGL8MJQHm3gm+/fkWj6fd/zZcMFb\nXWks3VZEweJtZxRfD6oQqgkJNJ2Xwwt3PchALZAg+259Nv2rT0bG1L/bwy6vnbFWL8t/tYgOacEu\ndOIVP3Yh2OOJp9pvMMnWxRG/QcweO+q7OyKmwxcWK4wcTMntVv5n0hqG2GpRkWx39+M35Rfzx/zn\nyNFULEJFAYbE1vHirEGkH7aZZZgRhlXxYxUGyueEzyJUdGnwsSBeHNVI6eQtvD6nCMeLO75xe0EV\nQs+oXFoucjFQC7itBgZydSKipvRffDIMkBK96QR3PHwbv71zKVPtbRzxGSxvnszaD0eRsN2KvUUy\n7ic7GZ28hXsqriC2Qv/a0/zfVoTFinvmCNS76nkqdx2bT57FzzbPJfaAhaS9blAEf1s0iYX9t+JU\nbTgVKzcmbCfuOhfrymcS+9LesE+5+jQWofJxaDQ/v46ulAFEyryxcrCCEw8UctG0u9HtkuhyBcXX\n/WJ3QnX75C7uG/sS82LqiVbdGJYzC6QGTQi11BTqRti4bug76N3Byv9pGkPSu00YrZExAyh9XtLX\nVXGP7/v4ogWqBxzNBnlVbqzllXgGpXJJfKC87tCxVHJOREaKiNA0mq4fQ/RVtTycv4pqfzyP75lC\n7koDe1k18mQnJb8o4O6Yg6hC8NumEZScTGG6s4Qb4vbx3h057B82iozXu7CccIFuINo6wnoxBv1T\ndeh3Zb3OjwpvJHNbEnpjYwit6h2Mjg4cWw6RV52OVFXUhhbQPzvwjS/L4KkBk/hu4ZqgtBk0IWye\nmYNtZiOXxe7GQLDNHcOa1dPIqtobMV4PgL+6hqQlNV84rttsuCdlUGT3AiqWww4sdQ1hnzeo2O24\nZg4ndUEFj+Y+z9KWCfxt10QGbLRg212CTHJy7LIM/nfW06Rpbfygcjb73izAUSfYkVPI+nEVzE7Z\ny+hLq3k8awqcTEDpEiTvTCRmZZgKodfH4pZ8bk8oAeAcx0m6hnXhHZqB+lb4CyEExJB9gVLD06mH\nPTmBFk/wCi+DJoT1M/y8PexJnIqVE4aXX5bOIfuP+7+QDR6xGBKt08AgEPdIOGQga2pDbVWPImw2\n5NA8Wm7qYH3+Gh5rHcGqtcXkvu3BWt+KZ1QudRNt3HzNBqxC566yK2lfPpC8NyrRG5pIS02mfXQG\nDxRncd25W1hZ9BesGKxqHce6lqnEhPoL9hDS7eGR7edy+6ySU8fGZFdRVlBA4luhs6vH6c6xNZpP\nfKnzpA1Mp+oCJxembg9a00ERQjU2FnuMhxTVgYHBCUPDvyYJvf1IME4fFkifF8eWj9ClRImQvDAl\nI42SBTF8NO4R6nWdJ1ZdQPoWN+5EC5UXxVM8dT8r0l6jQwoueepH5K5sIeHwLvzdEyP+6hoc1TXk\nvwgv31DMylljsFr8aBviydlQeVpPIRyQXi/Rhy24DB9RigUFhRjNg24ToKjht1q1ECg2G0pSIsdn\nZ5L2Qhn+hqYvfk8hUKKiqLwmi1uvf4kFsaWAhiHPPPE8KEJY+ssh3DtszakAr8tQSH6nKeyHfV8H\nxW6ndc4wVPFWqE3pNTxZTp6eE1hI0y4E37tyE9FXu5keVUqOFiiSr/QrzF55F3m/eh/jS7wA55Pb\ncT75yd/hKoIARmcnab9/j+dvyGdBTAWKgKWZWxk1cyDqa1noh79ZrlxfRe3vpOX8QQz49zLezVvM\nxftvQWvvwPhUhYhQVdTUFI5dnsVDN/+VyfYOLCJQr17rjUNznVkm7hkLoZw0gvGTSpgRdRSXYeVV\nVyI/3jCfgqoPz/TU4YXFQltOZJVMKV6Dba5BjLKVEKdYWej8CIBFzSNZdmAitg/6MXBTC3kffRBR\nceSvhDRYUTWBS4ccJk4IfBIMGZ5DCd9ZmVx5z+unYqKVt+r4rj4boX/yfaXF4Krx77MmeV33EZXH\n23JZtON80tdrRG1474xsOGMh9MZbGRFbQ4pqo8bvYXH5uQz+5UdmbPBzSLeHAe960G5TQ21Kr6G9\nf5BN10xg6XcuxHO2C+OEjYyNkn4lzeS3HEd2dmK4PeE31AsGUtK1MpXm+wRxEdB/KuITj27z5Efx\nneY9cYqKgpXdXoOflV3Byb+lU7ixDKO17Yxzcc98aKyARegoKOgIvLqK3t5+xqcNN6TPi23XUV5x\nRTPT0YorWSEhPi6sq0oMtxvxURnZzUnI2H7gbUPWN6F3ukzx+wr0/7CDVW1juSVhB4lq+GYRWo/W\ns2zJLF69tIr1hWtPLdT7MW2Gl23uFJr90fy1fCruV5NJ/qCT/qWlQdv87YyF0HGsk1frh3JR9H7U\n8PTcg4be2sqdW+bz9nl/QD+nlda6TOL+4Qvr3DDp8wZ2ojP52ihlx3jloWLWOKdjaBBfpkP9wVCb\nFXT0hibSX9Joq01nzPzvsWLUE+RbJBtdyTzXMI6dR7Pot8+OrUUSXeun/4dV6LV16EEMp5yxEIrq\nOhrXFTJryEKwGESXWInDnC0+LVKSsV5h1YThLCx8k/vPvxhLRzb2TS1mjMzkC+gtLSQs/2yKSDj6\n0dLnxV9RRfSxOlT3SC6/8A6I8aHV2og5CrmHPVj3HQp4f/TMRNkZC6HefIKUR7aREgxrIoDofxzi\nT5vP49biv3PduO0821hE/o7YUz+yiUmkIn1e7C/v4HQb+PV0BxABYdi+hd7eTuHPD/Hkyguo6nJi\nZHVBojPUZpmYRDS9tp2nySforW1k3L+N4/dDHnvCcrhjYvJtQsgw3yzIxMTE5F9hDo1NTEwinp4R\nQiHOQog3EaINIY4gxGU90s63BSFsCLEUISoRogMh9iDERaE2K+QI8QOE2IkQHoRYFmpz+gxCrECI\nWoRoR4hShLgp1CaFlF64T4IvhEJowIvAy4ATuAVYgRAFQW/r24MGVAPFQBxwL7AKIbJDaFNf4Dhw\nP/BEqA3pY/wvkI2UscBs4H6EGBNim0JJj98nPeERFgJpwMNIqSPlm8A7wLU90Na3Ayk7kfJXSFmB\nlAZSvgyUA5F8c4OUa5ByHdAcalP6FFIeQMqP9yaQ3f/yQmhRaOmF+6S3YoQCOLuX2ur7CJECFAAH\nQm2KSR9FiEcRwgUcAmqBDSG2KKzpCSEsARqAuxHCghDnExgSRvVAW98+hLAATwPLkfJQqM0x6aNI\neRsQA0wD1gDm7lU9SPCFUEofcCnwHaAOuAtYBXxx/fpIQwgFeArwAj8IsTUmfZ1AaGkrMBC4NdTm\nhDM9k1At5T4CXmAAIbYBy3ukrW8LQghgKZACzOruMExMvgoakRwj7AV6Kn1mOELYESIKIX4EDACW\n9Uhb3x7+DJwFXIKUXf/qzRGBEBpC2AEVULvvmciudhIiGSGuRohohFAR4gJgHvBGqE0LGb1wn/TU\nZMm1BAK8DcC5wHmfmgWLPITIAv4NGAnUIcTJ7n/XhNiyUHMv0AX8FFjQ/f97Q2pR6JEEhsE1QAvw\ne2AhUq4PqVWhpcfvE7PEzsTEJOIxS+xMTEwiHlMITUxMIh5TCE1MTCIeUwhNTEwini+dgj5PuTKk\nMymbjOf73HZQ5jU5PeZ1+SLmNfkiffWamB6hiYlJxGMKoYmJScQT2Vn8JiFF7e/k5LR8ms/SsJ+Q\npK4vx1/faG7+btLrmEIYChQVYdFQ4uMQUQ7w64FN0CMpuV1R8RdkwK2NHBi2lpUdCTxWfgW29g6M\nzs5QWxcShKYhbDaERUMkxNNZmHz690mwNXUhPzgQOfeMoqI64/EXDETZcSDo+4CbQtiLCE1DiYuF\n/gno8VHUj4qmPRcs7YKcpT78dfWhNrHXUONiac6P4pGCx/FJhcuiG/hdtoWU3VEQaUKoqCgOO2Lg\nALqy4/FHqdSPVzh43f8BYPBZsWsz3FxT+l2069PwH68Lfw9aCNS4WBpnFzDl9vc5fHEi/tq6oDbR\nc0IoBIhPhSCl8dnXI6Un60ZYrIih+ZTebePqoTu5xbmdFNUGwFa3nZvTbmLQ7ZEjhMbJTmKr3PxX\n5WzWnm5H7whCOXsQ1Rc5STz3OP+Z8wz9FA9T7W6M7hC+wWefnRjFytrBaxi3+AayfmLFqKgOuofU\nlxCaBSN/IO/+5v94x6Pw3xnXIk60ID3BW74g6EKoJiRwsmgQNVf4uWnkO9yasBub0Ji2ewHetxNB\nwsl8HzkvSCyv7wx2830ONSWZjik5NM/v5LXxS0hRbSgobPdEs6kzlQxLM+Nsbfz4nJdZN/Yc5M79\noTa5V5A+L5YmF6W1yTAo1NaElkO3xfLAjKeZ06/pK39GFYJdE5dRPOE/cDa3ore09KCFfYcpNgNX\nehQxh2zofVUI1aGDKZ/bn3vnP0exo5J4RePe+iLSbS0sGrKatOEdALilylUJN5O1SYStZ6jGxlJ7\n7dnEzK7lF3lPMMzagrPbA1zb6eTB380n/rCHo3MtHLzsT4x2VLAyyYEtxHb3Fordjjs9hmk55iLd\n8R9qbB1bwJx+TbgMH5vdyfxw89WBDS66H4+MjGa+n/UO82KOhdTWUKKKnktyCY4QKipqfjblc/vz\nk3mrKXZU8mjzZNatnkraNjf7VMGj353B/UVruCq6gRp/F/7qfkFpuq/SWVyI7TsN/HnwM2RpAosI\nSNxGVxwPPjCflI1VYLWguFMBsGJEVjKTxYI3VmVafOmpQ22DoH9+GqrbjdHREULjepe09VXsOzqS\noqSxCENicUnO2v9Z71B3xnD/3MsZPfcPDLaoANxYeR7OPS0Rc610aaAKBd0mQFWDeu6gCKExeRhH\n5tpZcM5mJjvKebixiM1/nkD2W7UYFdXok4bhSPAw1HoclzR4z53BwDf0sPUGAerGq/xb5i7yLRou\nw8dOTxQvtY7ktdUTyX6tAn9tPWJEIXpMmAe6/wnS7SGq1sNfyqdxzfBVAFx7/mZWnZhOVmsqHIyM\nhxvAX3MMR/MJoqwWAKTPj/65CSNleCGGJQq70AEVXUrer84kr6E2rOODn0eXBh2ZCglRDghiOOCM\nhVCfPpqyqzV+VryeMfYK7q2ezYFXBpP57N5TP2bNDAffG7yVs6wKuz0a934wh4LtRwhnCXA0CJ6t\nGMuu9kwONKbSejyWmCMa2c8cDcwOS4nPaccS6w21qSFB+rxoR47jfikfhgeO/TRxL8uyp6DHOUJr\nXG8jJYbLBa7Tv6wW5FFb5CRryDGSlE8qxBxbo5GRMsNuGLQbbmIVO52ZOtIR3CDSGQmhWpDHkRsl\nK6YsIU11cXfVpZQ9V0Dm8g8DuWBCoCYnMbComrmxu6nxw0PH55CxXAv74G7aysO01+ZyOKo/KQfa\nGVBVjt7YyKf7brdTwxnXFjIbQ41e30DaKzaUnwsiKy7w1dAGpuPNTqJ6ehQJU+u4M/sNopSA11jp\n95P2bAl6V/jv+iB1HbXLxzaPkwsdLhSnB7q952DxzYVQUTl0ZyJbihcBcN/xCyldV0D647sw3G6E\npqFmpFN7YRo3pG4gRhH8/Pj5lD43mJSN24Jlf59Fb2yk3+pGIBDv1gElJqZ7D6cA7Tkq5yRXoktJ\nq+FA8YVvqODLUIWCIbvHB31umYBeQFFR7DaEww5a4JEUQlBxbRbz5r3JvLidDFCtqELgln6O+FQe\nOHYJ0uMN6/DSKQwdGlv4Y+VMLizsmR0LvpkQCoGWksSyi/6CU7Fye825lD40lAGrtmEIgbDZYHgB\nhUsOsCL5WaIUC79smMK2dSPIfGLv57KiIgM1Po7yhUPxO+Sphz1xWD3FsSVU+v38+uglWP++O7RG\nhgif1D/JlYuA5/rzqLmZ1F4wgM6pnRTnHgFAEQbr0h8BwOjOJWgzvDzWMobnl84gfUMdRsfRkNkc\nbnxzj9BuY7TVjSpUDiw5m6SdtZy8ZDxVs2DO+F3clriEHM0O2DAweH7faPLedkVU+ZRityMy0mic\nlkL69UfZk78YAA31VLVArd7FworLsP/QgRHuFQImX2T8MJQHm3gm+/fkWj4/3PtsuOCtrjSWbiui\nYPG2sI6v/yuccZ1IS3BToL/52dwednntjLV6Wf6rRXRIC3ahE6/4sQvBHk881X6DSbYujvgNYvbY\nUd/dEREdvrBYYeRgSm638j+T1jDEVouKZLu7H78pv5g/5j9HjqZiESoKMCS2jhdnDSL9sC2o2fIm\n3w6sih+rMFA+J3wWoaJLg48F8eKoRkonb+H1OUU4XtwRAkv7Bo8NWcGPnLcSzASabyaEUqI3neCO\nh2/jt3cuZaq9jSM+g+XNk1n74SgStluxt0jG/WQno5O3cE/FFcRW6BExzS8sVtwzR6DeVc9TuevY\nfPIsfrZ5LrEHLCTtdYMi+NuiSSzsvxWnasOpWLkxYTtx17lYVz6T2Jf2Yrjdof4avYpFqHwcHs3P\nr6MrZQCRMm+sHKzgxAOFXDTtbnS7JLpcQfF1v9idUN0+uYv7xr7EvJh6olU3hiUCA6k+L0f25WEU\nSuIUH1IJ7jX4xh6h9HlJX1fFPb7v44sWqB5wNBvkVbmxllfiGZTKJfGB8rpDx1LJORH+aSJC02i6\nfgzRV9XycP4qqv3xPL5nCrkrDexl1ciTnZT8ooC7Yw6iCsFvm0ZQcjKF6c4Sbojbx3t35LB/2Cgy\nXu/CcsIFuoFo6wj7xRj0T9Wh35X1Oj8qvJHMbUnojY0htKp3MDo6cGw5RF51OlJVURtaQP/swDe+\nLIOnBkziu4VrQmRl6JE+P9FVAc/YIsCToGGNigqkHQWBMxpo+6trSFpS84Xjus2Ge1IGRXYvoGI5\n7MBS1xDWcQ3Fbsc1czipCyp4NPd5lrZM4G+7JjJgowXb7hJkkpNjl2Xwv7OeJk1r4weVs9n3ZgGO\nOsGOnELWj6tgdspeRl9azeNZU+BkAkqXIHlnIjErw1gIvT4Wt+Rze0IJAOc4TtI1rAvv0AzUt8Jf\nCCEghuwLlBqebsxkT06gxRMpxZf/BCmxdASGDXYh8MQoxDrs0BeE8J9iSLROA4NA3CPhkIGsqe2R\npvoCwmZDDs2j5aYO1uev4bHWEaxaW0zu2x6s9a14RuVSN9HGzddswCp07iq7kvblA8l7oxK9oYm0\n1GTaR2fwQHEW1527hZVFf8GKwarWcaxrmUpMqL9gDyLdHh7Zfi63zyo5dWxMdhVlBQUkvhU6u3qc\n7hxbo/nEl4aMtIHpVF3g5MLU7b1oXN9D6jpRTTr1ehf9hEJXikDExULziaCcv0eEUPq8OLZ8hC4l\nQR7K90mUjDRKFsTw0bhHqNd1nlh1Aelb3LgTLVReFE/x1P2sSHuNDim45KkfkbuyhYTDu/B3T4z4\nq2twVNeQ/yK8fEMxK2eNwWrxo22IJ2dD5Wm9hHBBer1EH7bgMnxEKRYUFGI0T6CeVFHDb609IVBs\nNpSkRI7PziTthTL8DU1f/J5CoERFUXlNFrde/xILYksBDUNGZuK59HhwrP+Apb8ez139d6FMbKFr\nVyKWoxVBOX+PCKFit9M6ZxiqeKsnTt/n8GQ5eXpOYBFNuxB878pNRF/tZnpUKTlaYG6r0q8we+Vd\n5P3qfYwv8QCcT27H+eQnf4ezCAIYnZ2k/f49nr8hnwUxFSgClmZuZdTMgaivZaEfDq9cObW/k5bz\nBzHg38t4N28xF++/Ba29A+NTFSJCVVFTUzh2eRYP3fxXJts7sIhAzXqtNw7NFYmZuIA0WLZzMhfN\n2IvzsWisb+4OWhZKzwyNLRbaciKn51K8BttcgxhlKyFOsbLQ+REAi5pHsuzARGwf9GPgphbyPvog\nImbOvzbSYEXVBC4dcpg4IfBJMGR4DiV8Z2Vy5T2vn4qJVt6q47v6bIT+yfeVFoOrxr/PmuR13UdU\nHm/LZdGO80lfrxG14b0QWN53MKSC4jWC+iz1zNDY7WHAux6024K7VE5fRXv/IJuumcDS71yI52wX\nxgkbGRsl/UqayW85juzsxHB7wm+YFyykpGtlKs33CeIioP9UxCce3ebJj+I7zXviFBUFK7u9Bj8r\nu4KTf0uncGMZRmtbROTifhlZWhfHi6zkHhuEfvBwUM7ZYzFC266jvOKKZqajFVeyQkJ8XNhWlRhu\nN+KjMrKbk5Cx/cDbhqxvQu90meL3Fen/YQer2sZyS8IOEtXwzSK0Hq1n2ZJZvHppFesL155arPdj\n2gwv29wpNPuj+Wv5VNyvJpP8QSf9S0vRm09ERm3xP0Mo3DbhH6hCYG8UiPbg6UmP7Vmit7Zy55b5\nvH3eH9DPaaW1LpO4f/jCNjdM+ryBnehMvhFK2TFeeaiYNc7pGBrEl+lQfzDUZgUdvaGJ9Jc02mrT\nGTP/e6wY9QT5FslGVzLPNYxj59Es+u2zY2uRRNf66f9hFXptHboZUgHgw4503rI2YWuVyCAWHvTc\n5k1SkrFeYdWE4SwsfJP7z78YS0c29k0tZpzM5AvoLS0kLP9sikg4+tLS58VfUUX0sTpU90guv/AO\niPGh1dqIOQq5hz1Y9x0KeH+E/2TZ10Ia7Fl9NtuTh5JbchLZ9W0QQiD6H4f40+bzuLX471w3bjvP\nNhaRvyP21I9sYhKpSJ8X+8s7ON0GfuHYAQQFKRmw6JMl/II5d96joWm9vZ3Cnx/iyZUXUNXlxMjq\ngkRnTzZpYmJi8rXp8Q3e9dY2Mu7fxvH7IY89Zm9nYmLS5xAykmehTExMTDA3ijAxMTExhdDExMTE\nFEITE5OIxxRCExOTiMcUQhMTk4jHFEITE5OI5/8BmVqaa8v/6XcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t60EaoycM3Px",
        "colab_type": "text"
      },
      "source": [
        "### ***QUESTION 03***\n",
        "\n",
        "---\n",
        "A. LOAD AND RESHAPE THE DATA.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsMdqXlXTfIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
        "#Normalize\n",
        "x_train = x_train/255.0\n",
        "x_test =  x_test/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb2T1QulNAB2",
        "colab_type": "code",
        "outputId": "b6c090c0-b033-4da4-e082-065f27915481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('ORIGINAL SHAPE OF THE DATA: ', x_train.shape)\n",
        "x_train = x_train.reshape(-1,784)\n",
        "x_test = x_test.reshape(-1,784)\n",
        "print('AFTER RESHAPING THE DATA: ', x_train.shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORIGINAL SHAPE OF THE DATA:  (60000, 28, 28)\n",
            "AFTER RESHAPING THE DATA:  (60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfNLWG8SNv7i",
        "colab_type": "text"
      },
      "source": [
        "B. PRINT LABEL OF FIRST IMAGE BEFORE AND AFTER CONVERTING TO THE CATEGORICAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUjdKzUmN6e2",
        "colab_type": "code",
        "outputId": "ee8ac87c-0dc9-4fb4-f1b9-fccba0b7d28b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('ORIGINAL FIRST LABEL: ', y_train[0])\n",
        "y_train= tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test= tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print('FIRST LABEL AFTER CATEGORICAL: ', y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORIGINAL FIRST LABEL:  5\n",
            "FIRST LABEL AFTER CATEGORICAL:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9jFyY4TTrD-",
        "colab_type": "text"
      },
      "source": [
        "### ***QUESTION 04***\n",
        "\n",
        "---\n",
        "A. BUILD BASIC SINGLE LAYER PERCEPTRON ON THE MNIST DATA.\n",
        "\n",
        "1. Single Dense 1024 Neurons\n",
        "2. Input Shape Being 784 Activation Function Relu.\n",
        "3. Softmax Layer With 10 Neuron As Output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV4aqSIbOkMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, InputLayer, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPaRJ24aiUp5",
        "colab_type": "code",
        "outputId": "ba135789-c924-4ec7-c6d5-794d6328d649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "slp_model = Sequential()\n",
        "slp_model.add(InputLayer(input_shape= [784]))\n",
        "slp_model.add(Dense(1024,activation= 'relu'))\n",
        "slp_model.add(Dense(10, activation= 'softmax'))\n",
        "slp_model.summary() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 814,090\n",
            "Trainable params: 814,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kIcOWCsgXP",
        "colab_type": "text"
      },
      "source": [
        "4. Loss- Categorical_Crossentropy\n",
        "5. Optimizer - RMSPROP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_FpSYSxscFF",
        "colab_type": "code",
        "outputId": "b5abd881-0e1c-4863-d492-6acd04174d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "slp_model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', \n",
        "              metrics = ['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXuuXGaFtQy9",
        "colab_type": "text"
      },
      "source": [
        "6. Batch_Size = 5000, Epochs=50\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBCJL5KktQFL",
        "colab_type": "code",
        "outputId": "a3144ad8-5f9c-4989-f470-ddc21d7ed230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "slp_model.fit(x_train, y_train, batch_size = 5000, \n",
        "          epochs = 50, validation_data = (x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.9796 - acc: 0.7033 - val_loss: 0.5281 - val_acc: 0.8326\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.4384 - acc: 0.8733 - val_loss: 0.3811 - val_acc: 0.8819\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.3476 - acc: 0.9001 - val_loss: 0.3255 - val_acc: 0.9047\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2989 - acc: 0.9126 - val_loss: 0.2485 - val_acc: 0.9289\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2546 - acc: 0.9276 - val_loss: 0.2262 - val_acc: 0.9350\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2282 - acc: 0.9342 - val_loss: 0.2093 - val_acc: 0.9379\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2051 - acc: 0.9400 - val_loss: 0.2150 - val_acc: 0.9340\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1845 - acc: 0.9468 - val_loss: 0.1592 - val_acc: 0.9539\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1753 - acc: 0.9480 - val_loss: 0.1628 - val_acc: 0.9532\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1518 - acc: 0.9565 - val_loss: 0.1450 - val_acc: 0.9573\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1360 - acc: 0.9609 - val_loss: 0.1275 - val_acc: 0.9631\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1367 - acc: 0.9593 - val_loss: 0.1244 - val_acc: 0.9638\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1214 - acc: 0.9651 - val_loss: 0.1231 - val_acc: 0.9639\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1082 - acc: 0.9688 - val_loss: 0.1257 - val_acc: 0.9633\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.1031 - acc: 0.9698 - val_loss: 0.1227 - val_acc: 0.9624\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0954 - acc: 0.9722 - val_loss: 0.1127 - val_acc: 0.9666\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0929 - acc: 0.9726 - val_loss: 0.1056 - val_acc: 0.9677\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0801 - acc: 0.9770 - val_loss: 0.1191 - val_acc: 0.9632\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0792 - acc: 0.9769 - val_loss: 0.0907 - val_acc: 0.9717\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0769 - acc: 0.9778 - val_loss: 0.0841 - val_acc: 0.9746\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0684 - acc: 0.9806 - val_loss: 0.0897 - val_acc: 0.9731\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0603 - acc: 0.9832 - val_loss: 0.0819 - val_acc: 0.9756\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0617 - acc: 0.9826 - val_loss: 0.0781 - val_acc: 0.9762\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0579 - acc: 0.9829 - val_loss: 0.0840 - val_acc: 0.9744\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0501 - acc: 0.9860 - val_loss: 0.0895 - val_acc: 0.9708\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0460 - acc: 0.9877 - val_loss: 0.0859 - val_acc: 0.9731\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0497 - acc: 0.9858 - val_loss: 0.0793 - val_acc: 0.9751\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0435 - acc: 0.9882 - val_loss: 0.0794 - val_acc: 0.9757\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0414 - acc: 0.9888 - val_loss: 0.0740 - val_acc: 0.9780\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0378 - acc: 0.9896 - val_loss: 0.0725 - val_acc: 0.9776\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0346 - acc: 0.9908 - val_loss: 0.0939 - val_acc: 0.9712\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0361 - acc: 0.9901 - val_loss: 0.0702 - val_acc: 0.9780\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0317 - acc: 0.9912 - val_loss: 0.0978 - val_acc: 0.9692\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0328 - acc: 0.9906 - val_loss: 0.0659 - val_acc: 0.9803\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0228 - acc: 0.9950 - val_loss: 0.0665 - val_acc: 0.9782\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0338 - acc: 0.9907 - val_loss: 0.0664 - val_acc: 0.9796\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0266 - acc: 0.9931 - val_loss: 0.0638 - val_acc: 0.9796\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0222 - acc: 0.9948 - val_loss: 0.1138 - val_acc: 0.9638\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0683 - val_acc: 0.9799\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0241 - acc: 0.9937 - val_loss: 0.0617 - val_acc: 0.9813\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0179 - acc: 0.9961 - val_loss: 0.0646 - val_acc: 0.9794\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0213 - acc: 0.9949 - val_loss: 0.0653 - val_acc: 0.9794\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0225 - acc: 0.9941 - val_loss: 0.0611 - val_acc: 0.9810\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0125 - acc: 0.9980 - val_loss: 0.0648 - val_acc: 0.9798\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0208 - acc: 0.9944 - val_loss: 0.0608 - val_acc: 0.9810\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0118 - acc: 0.9980 - val_loss: 0.0702 - val_acc: 0.9785\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0178 - acc: 0.9956 - val_loss: 0.0611 - val_acc: 0.9808\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0201 - acc: 0.9949 - val_loss: 0.0736 - val_acc: 0.9771\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0096 - acc: 0.9985 - val_loss: 0.0651 - val_acc: 0.9808\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.0173 - acc: 0.9952 - val_loss: 0.0603 - val_acc: 0.9816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f077c4f6630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfWiQUsk7Sp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slp_model.save('slp.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJbJNMUlGKaT",
        "colab_type": "text"
      },
      "source": [
        "10. PRINT ACCURACY AND LOSS OF SLP MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuvJhPAw7ZJP",
        "colab_type": "code",
        "outputId": "84905c40-a815-40e3-973e-d12e68678ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "loss, accuracy = slp_model.evaluate(x_test, y_test, verbose= 0)\n",
        "print('Test Loss: ', loss)\n",
        "print('Test Accuracy: ', accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss:  0.06029947560108267\n",
            "Test Accuracy:  0.9816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGBSpcWJHG0s",
        "colab_type": "text"
      },
      "source": [
        "### ***QUESTION 05***\n",
        "\n",
        "---\n",
        "1. Single dense layer 1024 neurons\n",
        "input shape being 784 activation function relu.\n",
        "2. Second Dense Layer 512 neurons and activation relu\n",
        "3. Third Dense Layer 512 Neurons and activation relu\n",
        "4. Softmax layer with 10 neurons as output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXsFg39RGdRP",
        "colab_type": "code",
        "outputId": "b6e75b54-a19d-4562-f32f-87045efc351b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "mlp_model = Sequential()\n",
        "mlp_model.add(InputLayer(input_shape= [784]))\n",
        "mlp_model.add(Dense(1024,activation= 'relu')) #first dense layer\n",
        "mlp_model.add(Dense(512,activation= 'relu'))#second dense layer\n",
        "mlp_model.add(Dense(512,activation= 'relu'))#third dense layer\n",
        "mlp_model.add(Dense(10, activation= 'softmax'))\n",
        "mlp_model.summary() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,596,426\n",
            "Trainable params: 1,596,426\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZILQpxzPDPm",
        "colab_type": "text"
      },
      "source": [
        "5. loss- categorical_crossentropy\n",
        "6. optimizer - adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht7sjnQSPEZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', \n",
        "                  metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVSpgezP_MB",
        "colab_type": "text"
      },
      "source": [
        "7. batch_size = 5000, epochs=50\n",
        "8. Print the accuracy and loss of MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPMrnWIhP-lO",
        "colab_type": "code",
        "outputId": "ddbb2f54-5058-4574-cbcd-6033961f7153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mlp_model.fit(x_train, y_train, batch_size = 5000, \n",
        "          epochs = 50, validation_data = (x_test, y_test)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.9516 - acc: 0.7424 - val_loss: 0.3818 - val_acc: 0.8910\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.3105 - acc: 0.9113 - val_loss: 0.2370 - val_acc: 0.9310\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.2089 - acc: 0.9403 - val_loss: 0.1760 - val_acc: 0.9467\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.1567 - acc: 0.9543 - val_loss: 0.1398 - val_acc: 0.9584\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.1225 - acc: 0.9647 - val_loss: 0.1212 - val_acc: 0.9637\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0986 - acc: 0.9714 - val_loss: 0.1011 - val_acc: 0.9689\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0782 - acc: 0.9775 - val_loss: 0.0911 - val_acc: 0.9726\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0645 - acc: 0.9816 - val_loss: 0.0835 - val_acc: 0.9738\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0522 - acc: 0.9850 - val_loss: 0.0763 - val_acc: 0.9754\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0433 - acc: 0.9874 - val_loss: 0.0709 - val_acc: 0.9775\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0356 - acc: 0.9905 - val_loss: 0.0701 - val_acc: 0.9776\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0312 - acc: 0.9915 - val_loss: 0.0662 - val_acc: 0.9791\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0251 - acc: 0.9937 - val_loss: 0.0659 - val_acc: 0.9779\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0204 - acc: 0.9951 - val_loss: 0.0657 - val_acc: 0.9789\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0164 - acc: 0.9962 - val_loss: 0.0639 - val_acc: 0.9796\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0127 - acc: 0.9978 - val_loss: 0.0618 - val_acc: 0.9800\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0100 - acc: 0.9983 - val_loss: 0.0622 - val_acc: 0.9806\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0080 - acc: 0.9989 - val_loss: 0.0646 - val_acc: 0.9809\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0066 - acc: 0.9991 - val_loss: 0.0665 - val_acc: 0.9801\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0053 - acc: 0.9995 - val_loss: 0.0649 - val_acc: 0.9807\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0040 - acc: 0.9998 - val_loss: 0.0659 - val_acc: 0.9803\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0034 - acc: 0.9998 - val_loss: 0.0667 - val_acc: 0.9807\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0028 - acc: 0.9999 - val_loss: 0.0669 - val_acc: 0.9813\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 0.9810\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0675 - val_acc: 0.9815\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0020 - acc: 0.9999 - val_loss: 0.0693 - val_acc: 0.9815\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0712 - val_acc: 0.9814\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0707 - val_acc: 0.9812\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0702 - val_acc: 0.9815\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0717 - val_acc: 0.9808\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0726 - val_acc: 0.9811\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0732 - val_acc: 0.9812\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 9.4309e-04 - acc: 1.0000 - val_loss: 0.0734 - val_acc: 0.9813\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 8.8398e-04 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 0.9812\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 8.2808e-04 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 0.9816\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 7.8237e-04 - acc: 1.0000 - val_loss: 0.0751 - val_acc: 0.9814\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 7.3839e-04 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 0.9813\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 7.0800e-04 - acc: 1.0000 - val_loss: 0.0761 - val_acc: 0.9813\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 6.7981e-04 - acc: 1.0000 - val_loss: 0.0763 - val_acc: 0.9814\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 6.4714e-04 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9816\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 6.1823e-04 - acc: 1.0000 - val_loss: 0.0773 - val_acc: 0.9814\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 5.9849e-04 - acc: 1.0000 - val_loss: 0.0777 - val_acc: 0.9817\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 5.7810e-04 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9821\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 5.6074e-04 - acc: 1.0000 - val_loss: 0.0784 - val_acc: 0.9814\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 5.4070e-04 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9819\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 5.2519e-04 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 0.9817\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 5.0953e-04 - acc: 1.0000 - val_loss: 0.0799 - val_acc: 0.9818\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 4.9716e-04 - acc: 1.0000 - val_loss: 0.0801 - val_acc: 0.9818\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 4.8558e-04 - acc: 1.0000 - val_loss: 0.0804 - val_acc: 0.9816\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 4.7465e-04 - acc: 1.0000 - val_loss: 0.0808 - val_acc: 0.9814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0775cc19e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG5qQNWgQUQo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yCgjtspQYrv",
        "colab_type": "code",
        "outputId": "d6adeab0-0eab-4c4f-fcb7-96d73ea588c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "loss, accuracy = mlp_model.evaluate(x_test, y_test, verbose= 0)\n",
        "print('Test Loss: ', loss)\n",
        "print('Test Accuracy: ', accuracy) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss:  0.08082253013303284\n",
            "Test Accuracy:  0.9814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehgrwSUekg8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_model.save('mlp.h5') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_b3noIxlFhk",
        "colab_type": "text"
      },
      "source": [
        "### ***Question 6:***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "A. Build basic Convolution neural network on the MNIST Data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALbqPfrIlSMC",
        "colab_type": "text"
      },
      "source": [
        "Reshape the Data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsMKB7DrlMIU",
        "colab_type": "code",
        "outputId": "fe210720-8b6c-4b7d-9575-f6facd03409f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
        "print('ORIGINAL SHAPE: ', x_train.shape)\n",
        "x_train = x_train.reshape((-1, 28, 28, 1))\n",
        "x_test = x_test.reshape((-1, 28, 28, 1)) \n",
        "print('AFTER RE-SHAPE: ', x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORIGINAL SHAPE:  (60000, 28, 28)\n",
            "AFTER RE-SHAPE:  (60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYwN4QIYm39H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalise\n",
        "x_train = x_train/255.0\n",
        "x_test =  x_test/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmtC3yivnXxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Categorical\n",
        "y_train= tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test= tf.keras.utils.to_categorical(y_test, num_classes=10) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxvYQ7_ytXTM",
        "colab_type": "text"
      },
      "source": [
        "1. Conv2D with 32 Neuron; Filter 3,3 ; Activation: Relu ; Stride\n",
        "(1,1)\n",
        "2. MaxPool2D ; Pool Size (2,2)\n",
        "3. Flatten the data again to send to desne layer\n",
        "4. 128 Neuron single Dense Layer with relu\n",
        "5. 10 neuron single dense layer with softmax as output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rjZkKrStZVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOVe0eDqukyE",
        "colab_type": "code",
        "outputId": "3db7f17e-def3-4654-8bb1-7b1b9603300c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "conv_model = Sequential()\n",
        "conv_model.add(InputLayer(input_shape = [28,28,1]))\n",
        "conv_model.add(Conv2D(filters= 32, kernel_size= (3,3), strides= (1,1), padding= 'same', activation= 'relu'))\n",
        "conv_model.add(MaxPool2D(pool_size= (2,2)))\n",
        "conv_model.add(Flatten())\n",
        "conv_model.add(Dense(128, activation = 'relu'))\n",
        "conv_model.add(Dense(10, activation= 'softmax'))\n",
        "conv_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 804,554\n",
            "Trainable params: 804,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjoI-qOuz81d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', \n",
        "              metrics = ['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyw0u2SI0F1D",
        "colab_type": "code",
        "outputId": "3cea5f43-0889-4c82-b3fa-cf27de029518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "conv_model.fit(x_train, y_train, batch_size = 5000, \n",
        "          epochs = 50, validation_data = (x_test, y_test)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 26s 433us/sample - loss: 1.1890 - acc: 0.6556 - val_loss: 0.5615 - val_acc: 0.8492\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.5008 - acc: 0.8538 - val_loss: 0.3986 - val_acc: 0.8879\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 24s 408us/sample - loss: 0.3720 - acc: 0.8894 - val_loss: 0.3581 - val_acc: 0.8923\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.3004 - acc: 0.9117 - val_loss: 0.3321 - val_acc: 0.8869\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 24s 408us/sample - loss: 0.2710 - acc: 0.9186 - val_loss: 0.3272 - val_acc: 0.8979\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.2274 - acc: 0.9326 - val_loss: 0.2315 - val_acc: 0.9302\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.2155 - acc: 0.9352 - val_loss: 0.1835 - val_acc: 0.9450\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 25s 409us/sample - loss: 0.1785 - acc: 0.9486 - val_loss: 0.2133 - val_acc: 0.9313\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.1649 - acc: 0.9511 - val_loss: 0.1635 - val_acc: 0.9515\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 24s 408us/sample - loss: 0.1488 - acc: 0.9574 - val_loss: 0.1311 - val_acc: 0.9613\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.1365 - acc: 0.9607 - val_loss: 0.1173 - val_acc: 0.9667\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.1216 - acc: 0.9646 - val_loss: 0.1111 - val_acc: 0.9661\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.1141 - acc: 0.9670 - val_loss: 0.0996 - val_acc: 0.9717\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.1063 - acc: 0.9688 - val_loss: 0.0926 - val_acc: 0.9718\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.0970 - acc: 0.9729 - val_loss: 0.0851 - val_acc: 0.9740\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 24s 408us/sample - loss: 0.0908 - acc: 0.9740 - val_loss: 0.0801 - val_acc: 0.9758\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0855 - acc: 0.9754 - val_loss: 0.0893 - val_acc: 0.9715\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.0785 - acc: 0.9772 - val_loss: 0.0797 - val_acc: 0.9747\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0682 - acc: 0.9808 - val_loss: 0.0828 - val_acc: 0.9735\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.0733 - acc: 0.9789 - val_loss: 0.0679 - val_acc: 0.9783\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 25s 409us/sample - loss: 0.0623 - acc: 0.9823 - val_loss: 0.0679 - val_acc: 0.9778\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0641 - acc: 0.9815 - val_loss: 0.0731 - val_acc: 0.9759\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0557 - acc: 0.9844 - val_loss: 0.0779 - val_acc: 0.9755\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.0563 - acc: 0.9839 - val_loss: 0.0631 - val_acc: 0.9791\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.0538 - acc: 0.9844 - val_loss: 0.0578 - val_acc: 0.9813\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0496 - acc: 0.9858 - val_loss: 0.0592 - val_acc: 0.9811\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0445 - acc: 0.9876 - val_loss: 0.0554 - val_acc: 0.9816\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0471 - acc: 0.9862 - val_loss: 0.0652 - val_acc: 0.9791\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 24s 408us/sample - loss: 0.0439 - acc: 0.9873 - val_loss: 0.0500 - val_acc: 0.9832\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0373 - acc: 0.9898 - val_loss: 0.0488 - val_acc: 0.9841\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0435 - acc: 0.9872 - val_loss: 0.0506 - val_acc: 0.9842\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0326 - acc: 0.9913 - val_loss: 0.0482 - val_acc: 0.9841\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0338 - acc: 0.9903 - val_loss: 0.0496 - val_acc: 0.9836\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0338 - acc: 0.9901 - val_loss: 0.0519 - val_acc: 0.9837\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 24s 403us/sample - loss: 0.0318 - acc: 0.9910 - val_loss: 0.0640 - val_acc: 0.9798\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 24s 403us/sample - loss: 0.0318 - acc: 0.9909 - val_loss: 0.0551 - val_acc: 0.9810\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.0275 - acc: 0.9925 - val_loss: 0.0527 - val_acc: 0.9824\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0305 - acc: 0.9911 - val_loss: 0.0507 - val_acc: 0.9827\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 24s 402us/sample - loss: 0.0259 - acc: 0.9928 - val_loss: 0.0442 - val_acc: 0.9853\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0251 - acc: 0.9931 - val_loss: 0.0524 - val_acc: 0.9822\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0240 - acc: 0.9935 - val_loss: 0.0472 - val_acc: 0.9848\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.0261 - acc: 0.9926 - val_loss: 0.0408 - val_acc: 0.9861\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.0198 - acc: 0.9949 - val_loss: 0.0650 - val_acc: 0.9779\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 24s 403us/sample - loss: 0.0246 - acc: 0.9931 - val_loss: 0.0437 - val_acc: 0.9861\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0222 - acc: 0.9937 - val_loss: 0.0448 - val_acc: 0.9851\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.0181 - acc: 0.9953 - val_loss: 0.0551 - val_acc: 0.9819\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0190 - acc: 0.9948 - val_loss: 0.0590 - val_acc: 0.9804\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 24s 403us/sample - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0436 - val_acc: 0.9861\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 24s 402us/sample - loss: 0.0177 - acc: 0.9950 - val_loss: 0.0481 - val_acc: 0.9862\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0150 - acc: 0.9966 - val_loss: 0.0628 - val_acc: 0.9805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0768895dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrBsInc91kcF",
        "colab_type": "text"
      },
      "source": [
        "SAVE THE MODEL AS .json, .h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e8lgSfH1e16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "model_json = conv_model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "conv_model.save_weights('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sIiTRG2BQ0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_model.save('conv_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m5OuQVw1qnv",
        "colab_type": "text"
      },
      "source": [
        "LOAD THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyt8CjnP1tvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json= json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights('model.h5')\n",
        "loaded_model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', \n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQt5KTXAC8rm",
        "colab_type": "text"
      },
      "source": [
        "LOSS AND ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-nMOP4VC78N",
        "colab_type": "code",
        "outputId": "86640db9-30cd-46dd-a4e4-fb4437618ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "loss, accuracy = loaded_model.evaluate(x_test, y_test, verbose = 0)\n",
        "print('Test Loss: ', loss)\n",
        "print('Test Accuracy: ', accuracy) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss:  0.06279286134364083\n",
            "Test Accuracy:  0.9805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mnU6GQbOZRe",
        "colab_type": "text"
      },
      "source": [
        "### ***Hence for MNIST Dataset even simple MLP is effective***"
      ]
    }
  ]
}